{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c75611-f49c-45ea-b301-772fc5246f8d",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2025\n",
    "\n",
    "In this homework we will solve constrained optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fa6b5-6922-4707-94ea-39701ae38ee1",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d6239-c320-42c7-a6b9-bdaa3677edd6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>Warning</b> You will need a couple of libraries for this homework, which you can install with pip by executing the block below.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    "    <b>Warning</b> Please use a python version earlier than 3.12. One recommended version is 3.10.19.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc1fa5-db8d-4358-aea0-dffa8ba8c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b9701-61ab-46ab-8dcf-2969b157121e",
   "metadata": {},
   "source": [
    "# 1. Constrained optimization - 40 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4132675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.deblur_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dab2f-86fa-4dc2-b556-12fe8abbc589",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997e534",
   "metadata": {},
   "source": [
    "We have seen in the lectures that given an optimization task whose iterates live on a  convex subset $\\mathcal{X} \\subset \\mathbb{R}^n$ we can ensure the iterates of our algorithms stay within $\\mathcal{X}$ in one of two ways. \n",
    "\n",
    "In the first way, we use projections, computed via the proximal operator $\\text{prox}_{\\delta_\\mathcal{X}}$. In the second way, we use linear minimization oracles  $\\text{lmo}_\\mathcal{X}$ within a conditional gradient framework\n",
    "that take simplicial combinations of elements from the set X whereby producing iterates remaining in $\\mathcal{X}$.\n",
    "\n",
    "<!-- The following exercises will help you understand what kind of computations are involved for each of these two operators, and how their computational complexity compares. For this we will work with $\\mathcal{X}$ being the set of low-rank matrices defined via the nuclear norm ball $\\mathcal{X}=\\lbrace X:X\\in \\mathbb{R}^{p\\times n}, \\Vert X  \\Vert_* \\leq \\xi \\rbrace$ with $\\xi$ being the radius of the zero-centered nuclear norm ball. -->\n",
    "\n",
    "We will first mathematically study properties of  the proximal operator $\\text{prox}_{\\delta_\\mathcal{X}}$  (**1.1** ) and get a feeling of its scalability (**1.2**).\n",
    "Then,  we do the same thing for $\\text{lmo}_\\mathcal{X}$ (**1.3 & 1.4**) \n",
    "In the end, we develop the Frank-Wolfe implementation for an image deblurring (**1.5**) problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34393988",
   "metadata": {},
   "source": [
    "## 1.1 Computing projections onto $\\mathcal{X}$ - 10 points\n",
    "\n",
    "\n",
    "#### Question 1.1.1 (2 pts)\n",
    "\n",
    "Recall that given a set $\\mathcal{X} \\subset \\mathbb{R}^{p \\times m}$, its corresponding\n",
    "    projection operator is given by\n",
    "    $\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z}) = \\mathop{\\mathrm{arg\\,min}}\\limits_{{\\bf X}\\in \\mathcal{X}}\\{ \\|{\\bf X}- \\boldsymbol{Z}\\|_F^2\\}, \\; \\forall \\boldsymbol{Z}\\in  \\mathbb{R}^{p \\times m}$.\n",
    "    Using the definition of the proximal operator given in class, show\n",
    "    the equivalence between the projection operator and the proximal\n",
    "    operator:\n",
    "    $$\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z})  = \\mathrm{prox}_{\\delta_{\\mathcal{X}}}(\\boldsymbol{Z}),$$\n",
    "    where $\\delta_{\\mathcal{X}}$ is the indicator function of\n",
    "    $\\mathcal{X}$ i.e.  $\\delta_{\\mathcal{X}}({\\bf Y}) = \\begin{cases} 0, \\text{ if } {\\bf Y}\\in \\mathcal{X} \\\\ +\\infty, \\text{ o.w. } \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b900b",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6030fe-e692-41e2-9ed8-51762f28c201",
   "metadata": {},
   "source": [
    "#### Question 1.1.2 (4 pts)\n",
    "\n",
    "Let $f:\\mathbb{R}^p \\to (-\\infty, +\\infty]$ be proper (i.e., $f$ is nowhere $-\\infty$ and finite somewhere), closed, and convex function. Denote by its Fenchel conjugate $f^*$.  \n",
    "\n",
    "Prove the Moreau’s identity, which states that: for every $\\mathbf{x} \\in \\mathbb{R}^p$,\n",
    "$$\n",
    "\\operatorname{prox}_{f}(\\mathbf{x}) + \\operatorname{prox}_{f^*}(\\mathbf{x}) = \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "(Hints 1: You may use without proof that $f^{**} = f$ for proper, closed, convex $f$.)\n",
    "\n",
    "(Hints 2: You need to first prove the following relationship:\n",
    "   For any $\\mathbf{x}, \\mathbf{\\theta} \\in \\mathbb{R}^p$:\n",
    "   $$\n",
    "   \\text{(a)}\\quad \\mathbf{\\theta} \\in \\partial f(\\mathbf{\\mathbf{x}})\n",
    "   \\quad\\Rightarrow\\quad\n",
    "   \\text{(b)}\\quad \\langle \\mathbf{\\theta}, \\mathbf{y} \\rangle - f(\\mathbf{y}) \\text{ achieves its supremum at } \\mathbf{y} = \\mathbf{x}\n",
    "   $$\n",
    "   $$\n",
    "   \\Rightarrow\\quad\n",
    "   \\text{(c)}\\quad f(\\mathbf{x}) + f^*(\\mathbf{\\theta}) = \\langle \\mathbf{\\theta}, \\mathbf{x} \\rangle.\n",
    "      \\quad\\Rightarrow\\quad\n",
    "   \\text{(d)}\\quad \\mathbf{x} \\in \\partial f^*(\\mathbf{\\theta})\n",
    "   $$)\n",
    "\n",
    "\n",
    "Next, use Moreau’s identity to find the proximal operator of $\\|\\mathbf{x} \\|_\\infty$ and $\\|\\mathbf{x} \\|_2$.\n",
    "\n",
    "(Hint 3: For $f(\\mathbf{x})=\\|\\mathbf{x}\\|_p$ with $p\\geq 1$, then $f^*(\\mathbf{x})=\\delta_{\\|\\mathbf{x}\\|_q \\leq 1}(\\mathbf{x})$ where $\\frac{1}{p}+\\frac{1}{q}=1$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014ecfa-3f3f-4a19-b8ec-cd6fd0c60df0",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b88e3",
   "metadata": {},
   "source": [
    "#### Question 1.1.3.  (4 pts)\n",
    "Let ${\\boldsymbol Z}= {\\boldsymbol U}\\boldsymbol{\\Sigma} {\\boldsymbol V}^\\top$ be the singular value decomposition of  ${\\boldsymbol Z}\\in \\mathbb{R}^{p \\times m}$. Denote the diagonal of $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{s \\times s}$ by a vector $\\boldsymbol{\\sigma} \\in \\mathbb{R}^{s}$, where $s = \\min \\{ p, m \\}$.\n",
    "\n",
    "Compute the proximal operator of the nuclear norm：\n",
    "$$\n",
    "\\operatorname{prox}_{\\|\\cdot\\|_*}({\\boldsymbol Z})\n",
    "=\\arg\\min_{{\\boldsymbol X}\\in\\mathbb{R}^{p\\times m}}\n",
    "\\Big\\{\n",
    "\\tfrac{1}{2}\\|{\\boldsymbol X}-{\\boldsymbol Z}\\|_{\\text F}^2+\\|{\\boldsymbol X}\\|_*\n",
    "\\Big\\}.\n",
    "$$\n",
    "\n",
    "\n",
    "(Hint: 1: You can use the conclusion from the previous two questions. The previously mentioned Moreau’s identity also holds for matrix norms. The conjugate of $f({\\boldsymbol X})=\\|{\\boldsymbol X}\\|_*$ is the function $f^*({\\boldsymbol Y})$ defined as $0$ if $\\|{\\boldsymbol Y}\\|_2 \\le 1$, and $+\\infty$ otherwise, i.e., it is the indicator function of the spectral-norm unit ball.)\n",
    "\n",
    "(Hint 2: Use Mirsky's inequality:\n",
    "    $\\| {\\boldsymbol X}- {\\boldsymbol Z}\\|_F \\geq \\| \\boldsymbol{\\Sigma}_{{\\boldsymbol X}} - \\boldsymbol{\\Sigma}_{{\\boldsymbol Z}}\\|_F$,\n",
    "    where\n",
    "    $\\boldsymbol{\\Sigma}_{{\\boldsymbol X}}, \\boldsymbol{\\Sigma}_{{\\boldsymbol Z}} \\in \\mathbb{R}^{s \\times s}$\n",
    "    are the diagonal matrices of the singular values of\n",
    "    ${\\boldsymbol X}, {\\boldsymbol Z}$ respectively.)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3d656",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5cc02",
   "metadata": {},
   "source": [
    "## 1.2 The scalability of $\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z})$ - 9 points\n",
    "\n",
    "In this exercise, we will get a sense of the execution time of\n",
    "$\\mathrm{proj}_{\\mathcal{X}} (\\boldsymbol{Z})$ in a realistic setup.\n",
    "We will carry out this compuation on two datasets provided\n",
    "to you in the codes. These datasets consist of the ratings given by\n",
    "MovieLens users to movies in a given list. The 100k dataset consists of\n",
    "100,000 ratings from 1000 users on 1700 movies. The 1M dataset consists\n",
    "of 1 million ratings from 6000 users on 4000 movies.\n",
    "\n",
    "As you likely figured out already from the numbers above, users do not\n",
    "rate all of the movies, and therefore, we model the ratings as entries\n",
    "of a low-rank matrix, where rows correspond to different users and\n",
    "columns correspond to different movies. A classical task in machine\n",
    "learning is to predict the value of the missing entries, which is called\n",
    "the matrix completion problem.\n",
    "\n",
    "Many other tasks can be formulated as convex minimization problems,\n",
    "constrained to the nuclear-norm ball, which captures a low rank model\n",
    "since it is the atomic norm of rank-1 matrices (see Lecture 4). A good\n",
    "optimization algorithm must ensure feasibility in a scalable way: For\n",
    "instance, the famous Netflix competition data consists of 100480507\n",
    "ratings that 480189 users gave to 17770 movies (much bigger than the\n",
    "datasets above). Projecting a matrix of this size onto the nuclear-norm\n",
    "ball is indeed demanding.\n",
    "\n",
    "#### Question 1.2.1 (3 pts)\n",
    "\n",
    "Implement the projection operator as a function called `proj_nuc` below. You can use the helper function `proj_L1` we define here from the `projL1.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.projL1 import projL1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267714eb",
   "metadata": {},
   "source": [
    "Set $\\xi = 3000$ and measure the computation time of the\n",
    "    projection operator with the 100k and the 1M MovieLens dataset using our provided helper code, which loads the datasets, constructs the data matrix, and times the evaluation of\n",
    "    the projection operator. Write the values you get in a markdown cell.\n",
    "    Run and report the average timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8858303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_nuc(Z, xi):\n",
    "    \"\"\" This function implements the projection onto the nuclear norm ball.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill all\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89219f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_completion(\"100k_MovieLens\", proj_nuc, xi=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce991cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This one can take few minutes!\n",
    "eval_completion(\"1M_MovieLens\", proj_nuc, xi=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88210fc",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dfb3f8",
   "metadata": {},
   "source": [
    "## 1.3 Computing the linear minimization oracle of $\\mathcal{X}$ - 2 points\n",
    "\n",
    "Problem 1.1 shows that projection onto the nuclear norm ball requires\n",
    "computing the singular value decomposition. The computational complexity\n",
    "of the singular value decomposition is $\\mathcal{O}(\\min(m^2p,mp^2))$,\n",
    "which can easily become a computational bottleneck if $m$ or $p$ are\n",
    "large. This bottleneck increased the popularity of algorithms that\n",
    "leverage the linear minimization oracle (lmo) instead (e.g.,\n",
    "[Jaggi2013](https://proceedings.mlr.press/v28/jaggi13.html), [yurtsever2018](http://proceedings.mlr.press/v80/yurtsever18a)):\n",
    "$$\\text{lmo}_{\\mathcal{X}}({\\boldsymbol Z})  = \\arg \\min_{{\\boldsymbol X}\\in \\mathcal{X}} \\langle {\\boldsymbol X},{\\boldsymbol Z}\\rangle \\qquad \\text{where}\\qquad \\langle {\\boldsymbol X},{\\boldsymbol Z}\\rangle = \\text{Tr}({\\boldsymbol Z}^\\top{\\boldsymbol X}).$$\n",
    "Note that $\\text{lmo}_\\mathcal{X}({\\boldsymbol Z})$ is not single valued\n",
    "in general. With abuse of terminology, when we say that we compute the\n",
    "lmo, we actually mean that we compute an instance ${\\boldsymbol X}$ such\n",
    "that ${\\boldsymbol X}\\in \\text{lmo}_\\mathcal{X}({\\boldsymbol Z})$.\n",
    "\n",
    "Show that the lmo$_\\mathcal{X}$ when $\\mathcal{X}$ is the nuclear norm\n",
    "ball:\n",
    "$\\mathcal{X} = \\{ {\\boldsymbol X}: {\\boldsymbol X}\\in \\mathbb{R}^{p \\times m} , \\| {\\boldsymbol X}\\|_\\ast \\leq \\xi \\}$\n",
    "gives the following output:\n",
    "$$-\\xi ~ \\! \\mathbf{u}\\mathbf{v}^T   \\in  \\text{lmo}_{{\\mathcal{X}}}({\\boldsymbol Z}) ,$$\n",
    "where $\\mathbf{u}$ and $\\mathbf{v}$ are the left and right singular\n",
    "vectors that correspond to the largest singular value of\n",
    "${\\boldsymbol Z}$.\n",
    "\n",
    "(Hint: By definition\n",
    "$\\xi ~ \\! \\mathbf{u}\\mathbf{v}^T \\in \\mathcal{X}$. You just need to\n",
    "show\n",
    "$\\langle {\\boldsymbol X},{\\boldsymbol Z}\\rangle \\geq \\langle -\\xi ~ \\! \\mathbf{u}\\mathbf{v}^T,{\\boldsymbol Z}\\rangle$\n",
    "for all ${\\boldsymbol X}\\in \\mathcal{X}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f1d86",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef3090",
   "metadata": {},
   "source": [
    "## 1.4 The scalability of lmo with Newton-Schulz - 10 points\n",
    "\n",
    "In this part, we will train a vision neural network for CIFAR 10. We use Scion as the optimizer, which take advantage of Newton-Schulz to compute the lmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f54e54",
   "metadata": {},
   "source": [
    "### 1.4.1 Introduction of Scion optimizer\n",
    "We have already seen the Scion optimizer in our last homework, in which you were asked to optimize a GAN with Scion. \n",
    "In this homework, we dig deeper into the details of this optimizer to better understand its advantage in efficiency. \n",
    "\n",
    "**What is Scion?**\n",
    "\n",
    "For neural network training, earlier progress in optimization converged to the following formulation of one-step update:\n",
    "$$W^{t+1}\\leftarrow W^t - \\eta^t H^t G^t,$$\n",
    "in which $W^{t+1}$ and $W^{t}$ are the parameters of a neural network before and after the update, $\\eta^t$ is a scalar learning rate, $H^t$ is a preconditioning matrix, and $G^t$ is some kind of average of the gradients.\n",
    "Such a formulation sits at the heart of adaptive methods, like ADAM, RMSProp, AdaGrad, etc.\n",
    "In this formulation, $G^t$ is a result of first-order information in the loss landscape. \n",
    "As this term throws away all the higher-order information, update merely based on itself might lead to unstability and other undesirable behavior.\n",
    "To tame this, people use a preconditioner $H_t$, which encodes higher-order information to \"reshape\" $G^t$ in a way that improves the performance.\n",
    "\n",
    "By contrast, instead of using the preconditioner, Scion makes use of lmo, where the \"constraints\" of the lmo reflects certain desirable \n",
    "properties of a well-behaved training process. \n",
    "For example, empirically, bounding certain norms of the updates normally yields better stability.\n",
    "Thus, Scion uses such norm constraints in its lmo.\n",
    "The update of Scion can be written as\n",
    "$$W^{t+1}\\leftarrow W^t + \\eta \\text{lmo} (G^t).$$\n",
    "Compared with the previous adaptive optimizer formulation, lmo yields a more straightforward guarantee."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851a393",
   "metadata": {},
   "source": [
    "### 1.4.2 Derivation of computation objectives (4 points)\n",
    "\n",
    "Let $G \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}$ have SVD\n",
    "$\n",
    "G = U \\Sigma V^\\top,\n",
    "$\n",
    "and $\\mathrm{rank}(G) = \\text{min}\\{d_{\\text{in}},d_{\\text{out}}\\}$.\n",
    "We know that the lmo using spectral norm yields $-U V^\\top$. \n",
    "One can get $U V^\\top$ from SVD, which we did in the last assignment. Nonetheless, SVD is too slow.\n",
    "Instead, we can use *Newton Schulz* method. \n",
    "We first derive what quantities Newton Schulz will help us compute by proving:\n",
    "\n",
    "1. \n",
    "   Suppose $d_{\\text{out}} \\ge d_{\\text{in}}$ and $\\mathrm{rank}(G) = d_{\\text{in}}$ (so $G$ has full column rank).  \n",
    "   Show that\n",
    "   $$\n",
    "   UV^T = G (G^\\top G)^{-1/2}.\n",
    "   $$\n",
    "\n",
    "2.\n",
    "   Suppose $d_{\\text{out}} \\le d_{\\text{in}}$ and $\\mathrm{rank}(G) = d_{\\text{out}}$ (so $G$ has full row rank).  \n",
    "   Show that\n",
    "   $$\n",
    "   UV^T = (G G^\\top)^{-1/2} G.\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91dda05",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"blue\">\n",
    "Put down your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f05b1c",
   "metadata": {},
   "source": [
    "### 1.4.3 *Newton Schulz* Iterations\n",
    "It turns out that Newton Schulz can help us compute $(G^\\top G)^{-1/2}$ and $(G G^\\top)^{-1/2}$ very quickly.\n",
    "Then, with the above result, we can easily obtain $UV^\\top$.\n",
    "\n",
    "---\n",
    "- **Input**\n",
    "    - $G \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}$\n",
    "    - Total iteration $T \\in \\mathbb{N}$\n",
    "- **Initialization**\n",
    "    - If $d_{\\text{out}} < d_{\\text{in}}$: \n",
    "        - $ S \\leftarrow G G^\\top$ \n",
    "    - Else:\n",
    "        - $ S \\leftarrow G^\\top G $ \n",
    "    - $N_0 \\leftarrow \\frac{S}{\\Vert S\\Vert_{F}}$, where $\\Vert \\cdot\\Vert_{F}$ denotes the Frobenius norm.\n",
    "    - $d \\leftarrow\\text{min}\\{d_{\\text{in}},d_{\\text{out}}\\}$\n",
    "    - $X_0 \\leftarrow I \\in \\mathbb{R}^{d\\times d}$\n",
    "\n",
    "- **Iteration** $t = 0,1,2,\\cdots,T-1$\n",
    "    - $U_{t+1}\\leftarrow \\left(3I -N_t\\right)/2$\n",
    "    - if $t>0$:\n",
    "        - $X_{t+1} \\leftarrow X_t U_{t+1}$\n",
    "    - else:\n",
    "        - $X_{t+1} \\leftarrow U_{t+1}$\n",
    "    \n",
    "    - if $t<T-1$:\n",
    "        - $N_{t+1}\\leftarrow N_{t} U_{t+1}^2$\n",
    "\n",
    "- **Output**\n",
    "    - $X\\leftarrow \\frac{X_{T}}{\\Vert S \\Vert_{F}^{\\frac{1}{2}}}$  \n",
    "\n",
    "---\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362bca1",
   "metadata": {},
   "source": [
    "After the above algorithm: $X \\approx (G G^\\top)^{-1/2}$ if $d_{\\text{out}} < d_{\\text{in}}$ otherwise $X \\approx (G^\\top G )^{-1/2}$.\n",
    "Then, with the result from the previous question, $UV^\\top$ is easy to obtain.\n",
    "Please implement the function below to compute $UV^\\top$ using the above Newton-Schulz method. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.compile\n",
    "def compute_UV(G, steps=9):\n",
    "    \"\"\"\n",
    "    Based on a newton-schulz flavor discussed in Lakic 1998.\n",
    "    :param G: The gradient of weights, 2-dimensional torch tensor.\n",
    "    :param steps: This \"T\" in the pseudocode \n",
    "    \"\"\"\n",
    "\n",
    "    orig_dtype = G.dtype\n",
    "    G = G.bfloat16()\n",
    "\n",
    "    # Implement the newton schulz method\n",
    "\n",
    "    # X = ???\n",
    "    # X should now store either (G G^T)^(-1/2) or (G^T G)^(-1/2)\n",
    "\n",
    "    # Then compute UV^T\n",
    "    O = ...\n",
    "    return O.to(orig_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844398d",
   "metadata": {},
   "source": [
    "Then we take your function for a spin. We are going to use it inside Scion, which relies on lmo for optimization, to train a neural network on CIFAR 10, a image classification task that contains 60,000 32x32 color images in 10 different classes. \n",
    "You can tell from the printout that the program below actually performs $5$ runs of the same training procedure (after a warmup run), each taking \n",
    "7 epochs.\n",
    "The warmup run is for \"initializing\" the GPU, which incurs one-off time-consuming operations to wire the resources together.\n",
    "The printout also contains information about the duration of each run. \n",
    "The entire time for this program should be below or close to 1 min, depending on the hardware.\n",
    "The validation accuracy averaged over the 5 runs (which is the number after \"Mean\" in the printout) should be around 0.94.\n",
    "Stop for one minute and appreciate how fast this training process is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.scion import *\n",
    "from lib.part1.airbench_scion_speedrun import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scion_cifar10(compute_UV=compute_UV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53be50",
   "metadata": {},
   "source": [
    "## 1.5 Frank-Wolfe for blind image deblurring - 15 points\n",
    "\n",
    "You are working with the local police to help identify a license plate\n",
    "of a car involved in a crime scene investigation. Unfortunately, the\n",
    "CCTV image of the car is blurry. In this exercise, we simulate this\n",
    "scenario with a deblurred license plate image found from the\n",
    "internet.\n",
    "\n",
    "Deblurring is an instance of the blind deconvolution problem: Given two\n",
    "unknown vectors $\\textbf{x},  {\\boldsymbol w}\\in \\mathbb{R}^L$, we\n",
    "observe their circular convolution\n",
    "$\\textbf{y}=  {\\boldsymbol w}*\\textbf{x}$, i.e.,\n",
    "$$y_\\ell = \\sum_{\\ell'=1}^L w_{\\ell'} x_{\\ell - \\ell' + 1},$$ where the\n",
    "index $\\ell - \\ell' + 1$ in the sum is understood to be modulo $L$.\n",
    "\n",
    "Blind deconvolution seeks to separate ${\\boldsymbol w}$ and\n",
    "$\\textbf{x}$, given $\\textbf{y}$. The operative word *blind* comes from\n",
    "the fact that we do not have much prior information about the signals.\n",
    "In this case, what we can assume is that ${\\boldsymbol w}$ and\n",
    "$\\textbf{x}$ belong to *known* subspaces of $\\mathbb{R}^L$ of dimension\n",
    "$K$ and $N$, i.e., we write $$\\begin{aligned}\n",
    "{\\boldsymbol w}&= {\\boldsymbol B}{\\boldsymbol h}, \\quad {\\boldsymbol h}\\in \\mathbb{R}^K \\\\\n",
    "\\textbf{x}&= {\\boldsymbol C}{\\boldsymbol m}, \\quad {\\boldsymbol m}\\in \\mathbb{R}^N\n",
    "\\end{aligned}$$ for some $L \\times K$ matrix ${\\boldsymbol B}$ and\n",
    "$L \\times N$ matrix ${\\boldsymbol C}$. The columns of these matrices\n",
    "form bases for the subspaces in which ${\\boldsymbol w}$ and $\\textbf{x}$\n",
    "live.\n",
    "\n",
    "As we have seen in Homework 1, natural images have sparse wavelet\n",
    "expansions. Hence, the image $\\textbf{x}$ can be expressed as\n",
    "$\\textbf{x}= {\\boldsymbol C}{\\boldsymbol m}$ with ${\\boldsymbol C}$ is\n",
    "the matrix formed by a subset of the columns of the wavelet transform\n",
    "matrix. In addition, the blur kernel ${\\boldsymbol w}$ is typically due\n",
    "to simple or \"sparse\" motion, which can be written as\n",
    "${\\boldsymbol w}= {\\boldsymbol B}{\\boldsymbol h}$ with ${\\boldsymbol B}$\n",
    "is the matrix formed by a subset of the columns of the identity matrix.\n",
    "\n",
    "In deblurring, $\\textbf{x}$ corresponds to the image we want to recover\n",
    "(i.e., the license plate) and ${\\boldsymbol w}$ to a 2D blur kernel.\n",
    "Thus, the 2D convolution $\\textbf{y}=  {\\boldsymbol w}*\\textbf{x}$\n",
    "produces a blurred image. We assume that we know or can estimate the\n",
    "support of the blur kernel (i.e., the location of its nonzero elements).\n",
    "In real applications, the support can be estimated by an expert using\n",
    "the physical information such as the distance of object to the focus and\n",
    "the camera, the speed of the camera and/or the object, camera shutter\n",
    "speed, etc.\n",
    "\n",
    "In this experiment, we use a very rough estimate for the support - a box\n",
    "at the center of the domain, whose size we have roughly tuned.\n",
    "Interestingly, it is possible to make the plate readable even in this\n",
    "setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b388a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib.part1.deblur_lib import setup_show\n",
    "setup_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5816b8",
   "metadata": {},
   "source": [
    "###  Reformulating the problem\n",
    "\n",
    "We now reformulate the blind image deconvolution problem, so that we can\n",
    "apply the constrained optimization algorithms we have seen in the\n",
    "course. Let ${\\boldsymbol b}$ be the $L$-point normalized discrete\n",
    "Fourier transform (DFT) of the observation $\\textbf{y}$, i.e,\n",
    "${\\boldsymbol b}= \\mathbf{F} \\textbf{y}$, where $F$ is the DFT matrix.\n",
    "Then, ${\\boldsymbol b}$ can be written as\n",
    "${\\boldsymbol b}= {\\boldsymbol A}({\\bf X})$ where\n",
    "${\\bf X}= {\\boldsymbol h}{\\boldsymbol m}^\\top$ and ${\\boldsymbol A}$ is\n",
    "a linear operator. Explicit expression of this linear operator\n",
    "${\\boldsymbol A}$ is out of the scope of this homework, c.f.,\n",
    "[Ahmed2014](https://ieeexplore.ieee.org/document/6680763/) for further details. This reformulation allows us to\n",
    "express $\\textbf{y}$, which is a nonlinear combination of the\n",
    "coefficients of ${\\boldsymbol h}$ and ${\\boldsymbol m}$, as a linear\n",
    "combination of the entries of their outer product\n",
    "${\\bf X}= {\\boldsymbol h}{\\boldsymbol m}^\\top$. Note that given\n",
    "${\\boldsymbol B}$ and ${\\boldsymbol C}$, recovering ${\\boldsymbol m}$\n",
    "and ${\\boldsymbol h}$ from ${\\boldsymbol b}$ is the same as recovering\n",
    "$\\textbf{x}$ and ${\\boldsymbol w}$ from $\\textbf{y}$.\n",
    "\n",
    "Since ${\\bf X}$ is a rank one matrix, we can use the nuclear norm to\n",
    "enforce approximately low-rank solutions. Then, we can formulate the\n",
    "blind deconvolution problem as follows:\n",
    "$$\\begin{align}\n",
    "{\\boldsymbol X}^\\star \\in \\arg \\min_{ {\\boldsymbol X}} \\bigg\\{ \\frac{1}{2} \\| \\mathbf{A}({\\boldsymbol X}) - {\\boldsymbol b}\\|_2^2 :  \\| {\\boldsymbol X}\\|_\\ast \\leq \\xi, ~{\\boldsymbol X}\\in \\mathbb{R}^{p\\times m}   \\bigg\\}, \\tag{4}\\label{eq:FWform}\n",
    "\\end{align}$$\n",
    "where $\\xi > 0$ is a tuning parameter.\n",
    "\n",
    "Note that our problem is constrained to the nuclear norm ball\n",
    "$\\mathcal{X} = \\{ {\\boldsymbol X}: {\\boldsymbol X}\\in \\mathbb{R}^{p \\times m} , \\| {\\boldsymbol X}\\|_\\ast \\leq \\xi \\}$.\n",
    "\n",
    "We will apply the Frank-Wolfe algorithm to solve the optimization\n",
    "problem given in \\eqref{eq:FWform}. The Frank-Wolfe algorithm is one of the earliest\n",
    "algorithms that avoids projections. Instead of projections, it leverages\n",
    "lmos (for a very good survey see [Jaggi2013](https://proceedings.mlr.press/v28/jaggi13.html)):\n",
    "$$\\mathrm{lmo}(\\nabla f ({\\boldsymbol Z})) = \\arg \\min_{{\\boldsymbol X}\\in \\mathcal{X}} ~ \\langle \\nabla f ({\\boldsymbol Z}), {\\boldsymbol X}\\rangle,$$\n",
    "where\n",
    "$\\mathcal{X} = \\{ {\\boldsymbol X}: \\| {\\boldsymbol X}\\|_\\ast \\leq \\xi, ~{\\boldsymbol X}\\in \\mathbb{R}^{p\\times m} \\}$\n",
    "as in Part 1. It applies to the generic constrained minimization\n",
    "template with a smooth objective function,\n",
    "$\\min_{\\boldsymbol X}\\{ f({\\boldsymbol X}) : {\\boldsymbol X}\\in \\mathcal{X}, \\, \\mathcal{X} \\text{ - convex, compact}  \\}$\n",
    "as follows:\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Frank-Wolfe's algorithm\n",
    "\n",
    "1. Choose ${\\boldsymbol X}^0 \\in\\mathcal{X}$.\n",
    "\n",
    "2. For $k=0, 1, \\ldots$ perform:\n",
    "  $$\\begin{cases}\n",
    "  \\hat{{\\bf X}}^k &:= \\mathrm{lmo}(\\nabla f ({\\boldsymbol X}^k)), \\\\\n",
    "  {\\bf X}^{k+1} &:= (1-\\eta_k){\\bf X}^k + \\eta_k\\hat{{\\bf X}}^k,\n",
    "  \\end{cases}$$ where $\\eta_k := {2}/{(k+2)}$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Question 1.5.1 (3 pts)\n",
    "\n",
    "Recall that the Frank-Wolfe algorithm applies only for\n",
    "    smooth objectives. Show that the objective function is smooth in the sense its gradient is Lipschitz continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d723d8",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740f510",
   "metadata": {},
   "source": [
    "### Implementation of Frank-Wolfe\n",
    "\n",
    "Complete the missing lines of the Frank-Wolfe update below.\n",
    "We provide you the linear operators that you need to compute the LMO in the code. Note that we do not need to\n",
    "    store and use the linear operator ${\\boldsymbol A}$ in the ambient\n",
    "    dimensions. In fact, for storage and arithmetic efficiency, we\n",
    "    should avoid explicitly writing ${\\boldsymbol A}$. You can find more\n",
    "    details about this aspect as comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe2f3e-bf73-4639-b7ac-5513b647944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part1.deblur_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf29bb-e52b-4487-83a9-59fab8eb953d",
   "metadata": {},
   "source": [
    "We will track three states:\n",
    "\n",
    "- `X`: corresponding to $\\boldsymbol X$\n",
    "- `AX`: corresponding to $\\boldsymbol{AX}$\n",
    "- `k`: the iterate count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744d821-7abd-416e-85b5-9a5681e2d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state():\n",
    "    return OptState(X=0.0, AX=0.0, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bcc19e-fd17-4389-939f-c982dd66de95",
   "metadata": {},
   "source": [
    "#### Question 1.5.2 (2 points) Complete the gradient computation of $f(\\boldsymbol X)$\n",
    "You have access to the following operators and variables:\n",
    "    \n",
    "- `A_T(Z)`: Computes $\\boldsymbol A^\\top \\boldsymbol Z$ for some $\\boldsymbol Z$.\n",
    "- `Aoper`: Computes $\\boldsymbol A \\boldsymbol X$ for 1-rank matrix $\\boldsymbol X$ using its SVD decomposition.\n",
    "- `b`: corresponds to $\\boldsymbol b$\n",
    "\n",
    "**Remark**: The `grad` method will receive `AX` and not `X` for computational efficiency reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3362772-7cdb-46ff-ac56-943c03c021aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Function(\n",
    "    f = lambda AX: 0.5*np.linalg.norm(AX - b, 2)**2,\n",
    "    grad = lambda AX: ???, # Fill\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011278e-7fc5-445d-8f3e-5bb1198421d2",
   "metadata": {},
   "source": [
    "#### Question 1.5.3 (3 points) Complete the LMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4901b8-5bb5-4595-abe0-b3c1d6be4c3e",
   "metadata": {},
   "source": [
    "You are allowed to use `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84a1ef-d64b-4e4c-a549-ef113280a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmo(Grad, xi=1000):\n",
    "    \"\"\" This function implements the lmo operator.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill\n",
    "    Xhat = ???\n",
    "\n",
    "    # Apply A to the rank 1 update\n",
    "    AXhat = Aoper(topLe_vec, -xi, topRe_vec.T)\n",
    "\n",
    "    return (Xhat, AXhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddb97b-f35e-40f2-8167-48f84ccdeb07",
   "metadata": {},
   "source": [
    "#### Question 1.5.4 (4 points) Complete the Frank-Wolfe update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a511f65-d0c3-447d-a493-af784f2f9d00",
   "metadata": {},
   "source": [
    "Fill in the missing update of `X` using the LMO from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd96d1-4ea0-41b1-a857-3276c9466c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(f, state):\n",
    "    X, AX, k = state\n",
    "\n",
    "    Xhat, AXhat = ???\n",
    "\n",
    "    # Step size\n",
    "    eta_k = ???\n",
    "\n",
    "    # Update X\n",
    "    next_X = ???\n",
    "\n",
    "    # Update A*X\n",
    "    next_AX = (1.0-eta_k)*AX + eta_k*(AXhat)\n",
    "\n",
    "    return OptState(next_X, next_AX, k+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46707353-179a-437e-b222-b1ad09786e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_algorithm = OptAlgorithm(\"FrankWolfe\", init_state, state_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd7055-e5cd-443d-91bb-2628c05d46e9",
   "metadata": {},
   "source": [
    "#### Question 1.5.5 (3 points) Run Frank-Wolfe\n",
    "\n",
    "Tune the \n",
    "    parameter $\\xi$ until the license plate number becomes readable.\n",
    "    What is the license plate number? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Frank-Wolfe's method\n",
    "xFW = run_frank_wolfe(f, opt_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eb9b5",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "Write your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05e205a5cd6e48f091f7f26f062202fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "06499b07396841d3bd39232291abefd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8ca44f5bb944418eaedeb2d086c9bcd4",
        "IPY_MODEL_43985f0e2db4410ba65a27756b0b791d",
        "IPY_MODEL_4564f5765442492d94dbd0e9161a9d2b"
       ],
       "layout": "IPY_MODEL_4db3bf7c5e0843b89743afda50540df0"
      }
     },
     "0b3abbd2d92d4145ba7acb48e32a8c48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0e2e483ba42f41e7b84fa9ddd91fc2ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_71fb36dc4e14447dbece28e50a89edf8",
        "IPY_MODEL_f51578e27d6c466285f22153a54fd385",
        "IPY_MODEL_39df6a566ec54b238158780513166dd4"
       ],
       "layout": "IPY_MODEL_2060a43bb3eb438195a0e3403ef99893"
      }
     },
     "1927ec5e0a514125835d381919ec19d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cc990b857eff4143be6ed9f529721e61",
       "style": "IPY_MODEL_d6530953075543c98fe3cc916e90e8cc",
       "value": "100%"
      }
     },
     "2060a43bb3eb438195a0e3403ef99893": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2192b12013c742c38500e84af40dd4b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_84907571ef8945058f781a3b416bff7d",
       "style": "IPY_MODEL_bfd258870ff445b99eafcc650c49ec1d",
       "value": " 5/5 [00:02&lt;00:00,  2.20s/it]"
      }
     },
     "254c308286e743fcad4f8a4808facd8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2af4922cf3044ac7bb290ab17ab0f47e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "33750796695443a083a59bb1c162d011": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "39df6a566ec54b238158780513166dd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f3758771a52a4cf9995dd0da9c62fbd9",
       "style": "IPY_MODEL_2af4922cf3044ac7bb290ab17ab0f47e",
       "value": " 201/201 [01:01&lt;00:00,  2.78it/s]"
      }
     },
     "3aa46b42fbf54fe785dc4f374c6005fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "43985f0e2db4410ba65a27756b0b791d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a278a3bd6e9a42918dd8d6e2b7387b93",
       "max": 5,
       "style": "IPY_MODEL_c936680039ed4bceb3888e2b25f51627",
       "value": 5
      }
     },
     "4564f5765442492d94dbd0e9161a9d2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cfe4da0bdd5f4394841a1e9947b68d49",
       "style": "IPY_MODEL_b8750f5a0764479fa86ce53b47c5d926",
       "value": " 5/5 [00:01&lt;00:00,  2.70it/s]"
      }
     },
     "4589847846c840dd8291643e75a5831a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4699ea5e261543eba0e653cb9d121b45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4c0511371b9f4bb6bee8e1074158e173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4db3bf7c5e0843b89743afda50540df0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4ec9a4fdf64344c89e94296edb3176f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "506379b1e582464080aa3442fc43142f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e4bb269164f54a09a393cc408f1b449e",
        "IPY_MODEL_c25c3aeedb984f518f700bf6d893cbf8",
        "IPY_MODEL_e3dc27f97bea443ea9fecec5f13e6afa"
       ],
       "layout": "IPY_MODEL_4589847846c840dd8291643e75a5831a"
      }
     },
     "56462a5b416148bd8d71b5187707a7a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "581c00ce52944a80a48625338434a0dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5987b378363349a09c6f77c532cbd134": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5ee52aa6813e43ffb886ece5739ebb9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "695c0b835dad4f178deb5e5ba948f685": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "71fb36dc4e14447dbece28e50a89edf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3aa46b42fbf54fe785dc4f374c6005fd",
       "style": "IPY_MODEL_e14e75706be047628040de8e11135d9a",
       "value": "201 | 1.3691e+04: 100%"
      }
     },
     "823ec206dc3e46938f01ec7f21c74a95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "84907571ef8945058f781a3b416bff7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "849f3959f86c4a7d862325286e6d6920": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "88c6976362ef4a468e80cf3d742b6b67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_eecac2443daa4ecc8700f6ea882eb190",
       "max": 5,
       "style": "IPY_MODEL_c1590aaa32e7408eb7b20c9aa42ad429",
       "value": 5
      }
     },
     "8b4dfbc30e194cdf8d10bcca4053d761": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8ca44f5bb944418eaedeb2d086c9bcd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_33750796695443a083a59bb1c162d011",
       "style": "IPY_MODEL_4c0511371b9f4bb6bee8e1074158e173",
       "value": "100%"
      }
     },
     "a278a3bd6e9a42918dd8d6e2b7387b93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a51e572492af48c7b03cd3059a630ea2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9d3b3f4abdb40bbbda58f991a717c34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1927ec5e0a514125835d381919ec19d6",
        "IPY_MODEL_88c6976362ef4a468e80cf3d742b6b67",
        "IPY_MODEL_bf6fa5dc19f54da88563e7a7262cf388"
       ],
       "layout": "IPY_MODEL_da309a8e827c443aab45009f90676a74"
      }
     },
     "b1bfc2f0d6e94ec6a25c42c39e21077d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4ec9a4fdf64344c89e94296edb3176f7",
       "max": 5,
       "style": "IPY_MODEL_0b3abbd2d92d4145ba7acb48e32a8c48",
       "value": 5
      }
     },
     "b8750f5a0764479fa86ce53b47c5d926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bf6fa5dc19f54da88563e7a7262cf388": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8b4dfbc30e194cdf8d10bcca4053d761",
       "style": "IPY_MODEL_56462a5b416148bd8d71b5187707a7a1",
       "value": " 5/5 [00:01&lt;00:00,  2.51it/s]"
      }
     },
     "bfd258870ff445b99eafcc650c49ec1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c062c13af6324281be74f9c403266ceb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f4dab36c58744d2a961bb4d02180e021",
        "IPY_MODEL_b1bfc2f0d6e94ec6a25c42c39e21077d",
        "IPY_MODEL_2192b12013c742c38500e84af40dd4b0"
       ],
       "layout": "IPY_MODEL_849f3959f86c4a7d862325286e6d6920"
      }
     },
     "c1590aaa32e7408eb7b20c9aa42ad429": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c25c3aeedb984f518f700bf6d893cbf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_823ec206dc3e46938f01ec7f21c74a95",
       "max": 5,
       "style": "IPY_MODEL_cb8fbcdb93074b5c81c75977f5b9b9c1",
       "value": 5
      }
     },
     "c936680039ed4bceb3888e2b25f51627": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb8fbcdb93074b5c81c75977f5b9b9c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cc990b857eff4143be6ed9f529721e61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cfe4da0bdd5f4394841a1e9947b68d49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d6530953075543c98fe3cc916e90e8cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da309a8e827c443aab45009f90676a74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e14e75706be047628040de8e11135d9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e3dc27f97bea443ea9fecec5f13e6afa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_695c0b835dad4f178deb5e5ba948f685",
       "style": "IPY_MODEL_5ee52aa6813e43ffb886ece5739ebb9d",
       "value": " 5/5 [04:11&lt;00:00, 49.96s/it]"
      }
     },
     "e4bb269164f54a09a393cc408f1b449e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_254c308286e743fcad4f8a4808facd8f",
       "style": "IPY_MODEL_581c00ce52944a80a48625338434a0dd",
       "value": "100%"
      }
     },
     "eecac2443daa4ecc8700f6ea882eb190": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3758771a52a4cf9995dd0da9c62fbd9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4dab36c58744d2a961bb4d02180e021": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_05e205a5cd6e48f091f7f26f062202fa",
       "style": "IPY_MODEL_a51e572492af48c7b03cd3059a630ea2",
       "value": "100%"
      }
     },
     "f51578e27d6c466285f22153a54fd385": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_5987b378363349a09c6f77c532cbd134",
       "max": 201,
       "style": "IPY_MODEL_4699ea5e261543eba0e653cb9d121b45",
       "value": 201
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
