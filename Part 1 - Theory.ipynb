{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1\n",
        "\n",
        "#### EE-556 Mathematics of Data - Fall 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we consider a multiclass classification task modeled by multinomial (softmax) logistic regression. Your goal will be to analyze the estimator and its properties (convexity, existence/uniqueness), and to derive gradients/Hessians and smoothness bounds. The first part consists of theoretical questions only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "  ‚ÑπÔ∏è <strong>Information on group based work:</strong>\n",
        "</div>\n",
        "\n",
        "- You are to deliver only 1 notebook per group.\n",
        "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
        "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know.\n",
        "- Only one member of the group is allowed to use AI. We will require sharing the conversation history with the AI in the form of a public link. If you use multiple conversations across the same or multiple tools please share all of them. Name the person in your group who is allowed to use AI. We encourage you to use the AI to help you understand the material, but we ask you to write the code and theory solutions by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
        "  ‚ö†Ô∏è Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Person 1 **Mathis Krause** || Person 1 **328110**:\n",
        "\n",
        "\n",
        "Person 2 **Youssef Hachouat**: || Person 2 **374574**:\n",
        "\n",
        "\n",
        "Person 3 **Emilien Silly**: || Person 3 **341507**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #0a0; background-color: #dfd; padding: 10px; border-radius: 5px;\">\n",
        "  üìì Feedback on AI use: Please use the following cell to provide feedback on the AI use in this notebook.\n",
        "  \n",
        "  For example, how useful were the tools to you? Which tools did you use? Did you feel like they helped you understand the material better?\n",
        "</div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used ChatGPT mainly for formating Latek expression after writing them by hand. It was very usefull to check correctness of our proofs and recall us basic properties that we forgot in part 1\n",
        "\n",
        "https://chatgpt.com/share/69049590-f8a8-800d-a326-19d5a669d0ca \n",
        "This is my discussion (Mathis) that benefited the group.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Multiclass Softmax Logistic Regression - 15 Points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now model multiclass classification with classes $c \\in \\{1,\\dots,C\\}$. For each sample $(\\mathbf{a}_i, b_i)$ with $\\mathbf{a}_i \\in \\mathbb{R}^p$ and $b_i \\in \\{1,\\dots,C\\}$, let $\\mathbf{X} = [\\mathbf{x}_1,\\dots,\\mathbf{x}_C] \\in \\mathbb{R}^{p\\times C}$ be the class weight matrix. The softmax model defines\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(b_i = c \\mid \\mathbf{a}_i) = \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_c)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}.\n",
        "$$\n",
        "\n",
        "Assume i.i.d. samples $\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n$. Our goal is to estimate $\\mathbf{X}$ by maximum likelihood (and later with an $\\ell_2$ regularizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(a)__ (1 point) Show that the negative log-likelihood $f$ can be written as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        " f(\\mathbf{X})\n",
        " &= - \\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
        " &= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "    f(\\mathbf{X}) \n",
        "    &= -\\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
        "    &= -\\log(\\prod_{i=1}^n \\mathbb{P}(b_i \\mid \\mathbf{a}_i )) \\text{ because }\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n \\text{ are i.i.d. samples}\\\\\n",
        "    &= -\\sum_{i=1}^n\\log(\\mathbb{P}(b_i\\mid \\mathbf{a}_i))\\\\\n",
        "    &= -\\sum_{i=1}^n\\log(\\frac{\\exp(\\mathbf{a}_i^\\top\\mathbf{x}_{b_i})}{\\sum_{k=1}^C\\exp(\\mathbf{a}_i^\\top\\mathbf{x}_k)})\\\\\n",
        "    &= -\\sum_{i=1}^n \\left[ \\log(\\exp(\\mathbf{a}_i^\\top\\mathbf{x}_{b_i})) -\\log(\\sum_{k=1}^C\\exp(\\mathbf{a}_i^\\top\\mathbf{x}_k)) \\right]\\\\\n",
        "    &= \\sum_{i=1}^n \\left[-\\mathbf{a}_i^\\top\\mathbf{x}_{b_i} +\\log(\\sum_{k=1}^C\\exp(\\mathbf{a}_i^\\top\\mathbf{x}_k))\\right]\\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(b)__ (2 points) Show that $\\mathbf{u} \\mapsto \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)$ is convex on $\\mathbb{R}^C$. Then, show that $f(\\mathbf{X})$ is convex.\n",
        "\n",
        "\n",
        "Hint: use Jensen's inequality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $h(\\mathbf{u}) = \\log\\left(\\sum_{k=1}^C e^{u_k}\\right)$.\n",
        "- We must show that for any $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^C$ and $\\theta \\in [0,1]$,\n",
        "$$\n",
        "f(\\theta \\mathbf{u} + (1-\\theta)\\mathbf{v})\n",
        "\\;\\le\\;\n",
        "\\theta f(\\mathbf{u}) + (1-\\theta) f(\\mathbf{v}).\n",
        "$$\n",
        "We start with the definition of convexity and use Jensen inequality\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    h(\\theta \\mathbf{u} + (1-\\theta)\\mathbf{v}) &= \\log\\left(\\sum_{k=1}^C e^{\\theta u_k + (1-\\theta)v_k}\\right)\\\\\n",
        "    &= \\log\\left(\\sum_{k=1}^C e^{u_k \\theta}e^{v_k (1-\\theta)}\\right)\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "Now,we can apply Holder's inequality (Analog of Jensen inequality for exponentials)\n",
        " with $p = \\frac{1}{\\theta}, q = \\frac{1}{1-\\theta}$, \n",
        "\n",
        "We can do that because $e^{u_k\\theta}$ and $e^{v_k (1-\\theta)}$ are positive\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\sum_{k=1}^C e^{u_k \\theta}e^{v_k (1-\\theta)} & \\leq (\\sum_{k=1}^C e^{u_k \\theta \\frac{1}{\\theta}})^\\theta (\\sum_{k=1}^C e^{v_k (1-\\theta) \\frac{1}{1-\\theta}})^{1-\\theta}\\\\\n",
        "    &= (\\sum_{k=1}^C e^{u_k})^\\theta (\\sum_{k=1}^C e^{v_k})^{1-\\theta}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        " $\\log$ is an increasing funtion so we can apply it to the previous inequality \n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\log\\left(\\sum_{k=1}^C e^{u_k \\theta}e^{v_k (1-\\theta)}\\right) & \\leq \\log\\left((\\sum_{k=1}^C e^{u_k})^\\theta (\\sum_{k=1}^C e^{v_k})^{1-\\theta}\\right)\\\\\n",
        "    &= \\theta \\log\\left(\\sum_{k=1}^C e^{u_k}\\right) + (1-\\theta)\\log\\left(\\sum_{k=1}^C e^{v_k}\\right)\\\\\n",
        "    &= \\theta h(\\mathbf{u}) + (1-\\theta)h(\\mathbf{v})\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus, $h$ is convex.\n",
        "\n",
        "To show that $f(\\mathbf{X})$ is convex we note that $-\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}$ is a linear map in $\\mathbf{X}$, thus convex.\n",
        "\n",
        "Finally, since $f(\\mathbf{X})$ is a sum and composition of convex functions, it is also convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator\n",
        "$$\n",
        "\\mathbf{X}^\\star_{ML} = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "is a global minimum. But does the minimum always exist? We will ponder this question in the following three points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(c)__ (1 point) Explain the difference between infima and minima. Give an example of a convex function on $\\mathbb{R}$ that does not attain its infimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "We consider a real-valued function  \n",
        "$$\n",
        "f : \\mathbb{R}^n \\to \\mathbb{R} \\cup \\{+\\infty\\}.\n",
        "$$\n",
        "\n",
        "\n",
        "The **infimum** of f, denoted $\\inf_x f(x)$, is the greatest lower bound of the set of values $\\{\\, f(x) : x \\in \\mathbb{R}^n \\,\\}$.\n",
        "\n",
        "It is the smallest number \\( m \\) such that  \n",
        "$$\n",
        "f(x) \\ge m \\quad \\text{for all } x \\in \\mathbb{R}^n.\n",
        "$$\n",
        "\n",
        "A **minimum** is *attained* when there exists a point \\( x^* \\)\n",
        "such that  \n",
        "$$\n",
        "f(x^*) = \\inf_x f(x).\n",
        "$$\n",
        "\n",
        "Hence, every minimum value is an infimum, but the converse is not true:  \n",
        "an infimum might not be reached by any \\( x \\).\n",
        "\n",
        "\n",
        "Consider  \n",
        "$$\n",
        "f(x) = e^x, \\quad x \\in \\mathbb{R}.\n",
        "$$\n",
        "\n",
        "Since  \n",
        "$$\n",
        "f''(x) = e^x > 0,\n",
        "$$\n",
        "the function \\( f \\) is **convex** on $\\ \\mathbb{R} $.\n",
        "\n",
        "We have  \n",
        "$$\n",
        "\\inf_{x \\in \\mathbb{R}} f(x) = 0,\n",
        "$$\n",
        "but there is **no** $x \\in \\mathbb{R}$ such that $e^x = 0$.  \n",
        "Therefore, $f$ does **not** attain its infimum.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(d)__ (1 point) Assume there exists $\\mathbf{X}_0 \\in \\mathbb{R}^{p\\times C}$ such that for all $i$,\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0.\n",
        "$$\n",
        "This is called one-versus-all complete separation in multiclass settings. Give a geometric interpretation (e.g., for $p=2$) and explain why the name is appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "Lets try to interpret it geometrically\n",
        "\n",
        "For $p = 2$, each column $\\mathbf{x}_{0,k}$ defines a **linear decision boundary** of the form  \n",
        "$$\n",
        "\\mathbf{a}^\\top \\mathbf{x}_{0,k} = c,\n",
        "$$\n",
        "which separates class $k$ from the others in the 2D feature space.\n",
        "\n",
        "The condition  \n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} > \\mathbf{a}_i^\\top \\mathbf{x}_{0,k}, \\quad \\forall k \\neq b_i,\n",
        "$$\n",
        "means that **every training point $\\mathbf{a}_i$ lies strictly on the correct side of all separating hyperplanes**:  \n",
        "each class can be perfectly separated from the others by linear functions of the form $\\mathbf{a} \\mapsto \\mathbf{a}^\\top \\mathbf{x}_{0,k}$.\n",
        "\n",
        "In the 2D case, this means that the samples of each class are contained in **non-overlapping convex regions** that are divided by straight lines (the decision boundaries).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this, you should see that it is likely that some datasets satisfy the complete separation assumption. Unfortunately, as you will show next, this can become an obstacle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(e)__ (1 point) In a one-versus-all complete separation setting (as in (d)), prove that $f$ does not attain its minimum. Hint: consider $f(\\alpha \\mathbf{X}_0)$ as $\\alpha \\to +\\infty$ and compare it to $f(\\mathbf{X}_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assume one-versus-all complete separation: there exists $\\mathbf{X}_0$ such that for all $i$,\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} - \\max_{k\\neq b_i}\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0.\n",
        "$$\n",
        "Let $m_{ik} := \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0$ for $k\\neq b_i$.\n",
        "\n",
        "Consider $f(\\alpha \\mathbf{X}_0)$ for $\\alpha>0$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\alpha \\mathbf{X}_0)\n",
        "&= \\sum_{i=1}^n \\Big[ -\\mathbf{a}_i^\\top(\\alpha \\mathbf{x}_{0,b_i}) + \\log \\sum_{k=1}^C \\exp\\big(\\mathbf{a}_i^\\top(\\alpha \\mathbf{x}_{0,k})\\big) \\Big] \\\\\n",
        "&= \\sum_{i=1}^n \\Big[ - \\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i}\n",
        "   + \\log \\Big( \\exp(\\alpha \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i})\n",
        "                 \\big[ 1 + \\sum_{k\\neq b_i} \\exp\\big(\\alpha(\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i})\\big) \\big] \\Big) \\Big] \\\\\n",
        "&= \\sum_{i=1}^n \\log \\Big( 1 + \\sum_{k\\neq b_i} e^{-\\alpha m_{ik}} \\Big)\n",
        "\\;\\xrightarrow[\\alpha\\to\\infty]{}\\; \\sum_{i=1}^n \\log(1+0) = 0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Next, for any $\\mathbf{X}$ and each $i$,\n",
        "$$\n",
        "\\log\\!\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\;\\ge\\; \\mathbf{a}_i^\\top \\mathbf{x}_{b_i},\n",
        "$$\n",
        "so the $i$-th summand in $f(\\mathbf{X})$ satisfies\n",
        "$$\n",
        "-\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log\\!\\sum_{k=1}^C e^{\\mathbf{a}_i^\\top \\mathbf{x}_k} \\;\\ge\\; 0,\n",
        "$$\n",
        "with equality only if $e^{\\mathbf{a}_i^\\top \\mathbf{x}_k - \\mathbf{a}_i^\\top \\mathbf{x}_{b_i}}=0$ for all $k\\neq b_i$ (i.e., infinite negative margins). Therefore\n",
        "$$\n",
        "f(\\mathbf{X}) \\;\\ge\\; 0 \\quad \\text{for all }\\mathbf{X},\n",
        "$$\n",
        "and equality ($f(\\mathbf{X})=0$) is unattainable for finite $\\mathbf{X}$.\n",
        "\n",
        "Combining:\n",
        "$$\n",
        "0 \\le \\inf_{\\mathbf{X}} f(\\mathbf{X}) \\le \\lim_{\\alpha\\to\\infty} f(\\alpha \\mathbf{X}_0) = 0,\n",
        "$$\n",
        "so $\\inf_{\\mathbf{X}} f(\\mathbf{X})=0$ but the minimum is not attained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We resolve this issue by adding a regularizer. Consider the regularized function\n",
        "\n",
        "$$\n",
        " f_\\mu(\\mathbf{X}) = f(\\mathbf{X}) + \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2, \\quad \\mu > 0.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(f)__ (1 point) Show that the gradient with respect to $\\mathbf{X}$ of $f_\\mu$ can be expressed as\n",
        "$$\n",
        " \\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n \\big( \\mathbf{p}_i - \\mathbf{e}_{b_i} \\big) \\mathbf{a}_i^\\top + \\mu \\mathbf{X},\\tag{1}\n",
        "$$\n",
        "where $\\mathbf{e}_{b_i} \\in \\mathbb{R}^C$ is the [one-hot vector](https://en.wikipedia.org/wiki/One-hot) for class $b_i$, $\\mathbf{p}_i \\in \\mathbb{R}^C$ has entries $p_{i,c} = \\mathbb{P}(b_i=c\\mid \\mathbf{a}_i)$ under the softmax model, and $(\\mathbf{p}_i - \\mathbf{e}_{b_i})\\mathbf{a}_i^\\top$ denotes the outer product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "We have\n",
        "$$\n",
        "f_\\mu(\\mathbf{X}) \\;=\\; \\sum_{i=1}^n\\!\\left[-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} \\;+\\; \\log\\!\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)\\right] \\;+\\; \\frac{\\mu}{2}\\,\\|\\mathbf{X}\\|_F^2,\n",
        "$$\n",
        "where $\\mathbf{X}=[\\mathbf{x}_1\\,\\dots\\,\\mathbf{x}_C]\\in\\mathbb{R}^{p\\times C}$.\n",
        "\n",
        "For a fixed $i$, define the logits $z_{i,k}=\\mathbf{a}_i^\\top \\mathbf{x}_k$ and the softmax\n",
        "$$\n",
        "p_{i,c} \\;=\\; \\frac{e^{z_{i,c}}}{\\sum_{k=1}^C e^{z_{i,k}}}.\n",
        "$$\n",
        "\n",
        "derivative w.r.t. a column $\\mathbf{x}_c$:\n",
        "-  the linear term:\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\mathbf{x}_c}\\!\\left(-\\,\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\\right)\n",
        "  \\;=\\; -\\,\\mathbf{a}_i\\,\\mathbf{1}\\{c=b_i\\}.\n",
        "  $$\n",
        "-  the log-sum-exp term:\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\mathbf{x}_c}\\log\\!\\sum_{k=1}^C e^{z_{i,k}}\n",
        "  \\;=\\; \\frac{1}{\\sum_k e^{z_{i,k}}}\\cdot e^{z_{i,c}}\\cdot \\frac{\\partial z_{i,c}}{\\partial \\mathbf{x}_c}\n",
        "  \\;=\\; p_{i,c}\\,\\mathbf{a}_i.\n",
        "  $$\n",
        "\n",
        "Therefore,\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial \\mathbf{x}_c}\n",
        "=\\sum_{i=1}^n \\big(p_{i,c}-\\mathbf{1}\\{c=b_i\\}\\big)\\,\\mathbf{a}_i\n",
        "\\quad\\Longleftrightarrow\\quad\n",
        "\\nabla_{\\mathbf{X}} f(\\mathbf{X})\n",
        "=\\sum_{i=1}^n (\\mathbf{p}_i-\\mathbf{e}_{b_i})\\,\\mathbf{a}_i^\\top,\n",
        "$$\n",
        "where $\\mathbf{p}_i=(p_{i,1},\\dots,p_{i,C})^\\top$ and $\\mathbf{e}_{b_i}$ is the one-hot vector for class $b_i$.\n",
        "\n",
        "**Regularizer gradient:**\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}}\\!\\left(\\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2\\right)=\\mu\\,\\mathbf{X}.\n",
        "$$\n",
        "\n",
        "**Putting it together:**\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X})\n",
        "\\;=\\;\n",
        "\\sum_{i=1}^n \\big(\\mathbf{p}_i - \\mathbf{e}_{b_i}\\big)\\,\\mathbf{a}_i^\\top\n",
        "\\;+\\;\n",
        "\\mu\\,\\mathbf{X}.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be written as\n",
        "$$\n",
        " \\nabla^2 f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I},\\tag{2}\n",
        "$$\n",
        "where $\\otimes$ is the Kronecker product, and $\\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top$ is the softmax Jacobian, which is positive semidefinite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "We saw that with $\\mathbf X=[\\mathbf x_1,\\dots,\\mathbf x_C]$ and $p_{ij}=\\mathbb P(b_i=j\\mid \\mathbf a_i)$,\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial \\mathbf x_j}\n",
        "=\\sum_{i=1}^n\\Big(-\\mathbf a_i\\,\\mathbf 1\\{b_i=j\\}+\\mathbf a_i\\,p_{ij}\\Big),\n",
        "\\qquad\n",
        "\\nabla_{\\mathbf X}f(\\mathbf X)=\\sum_{i=1}^n(\\mathbf p_i-\\mathbf e_{b_i})\\,\\mathbf a_i^\\top.\n",
        "$$\n",
        "\n",
        "**Block Hessian.** we differentiate w.r.t. $\\mathbf x_l$:\n",
        "- Only $p_{ij}$ depends on $\\mathbf x_l$.\n",
        "- Using the softmax Jacobian $\\frac{\\partial p_{ij}}{\\partial z_{im}}=p_{ij}(\\delta_{jm}-p_{im})$ and $z_{im}=\\mathbf a_i^\\top\\mathbf x_m$,\n",
        "$$\n",
        "\\frac{\\partial p_{ij}}{\\partial \\mathbf x_l}\n",
        "=\\sum_{m=1}^C\\frac{\\partial p_{ij}}{\\partial z_{im}}\\frac{\\partial z_{im}}{\\partial \\mathbf x_l}\n",
        "=\\frac{\\partial p_{ij}}{\\partial z_{il}}\\,\\mathbf a_i\n",
        "=p_{ij}(\\delta_{jl}-p_{il})\\,\\mathbf a_i.\n",
        "$$\n",
        "Thus the $(j,l)$ **block** ($p\\times p$) of the Hessian is\n",
        "$$\n",
        "\\frac{\\partial^2 f}{\\partial \\mathbf x_j\\,\\partial \\mathbf x_l}\n",
        "=\\sum_{i=1}^n \\mathbf a_i\\mathbf a_i^\\top\\,p_{ij}(\\delta_{jl}-p_{il}).\n",
        "$$\n",
        "\n",
        "Equivalently, if we collect the $C\\times C$ blocks into one operator, the per-$i$ contribution has $(j,l)$-block\n",
        "$$\n",
        "\\big[\\operatorname{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\big]_{jl}\\;\\mathbf a_i\\mathbf a_i^\\top\n",
        "= p_{ij}(\\delta_{jl}-p_{il})\\,\\mathbf a_i\\mathbf a_i^\\top,\n",
        "$$\n",
        "so in kronecker forme\n",
        "$$\n",
        "\\nabla^2 f(\\mathbf X)\n",
        "=\\sum_{i=1}^n\\big(\\mathbf a_i\\mathbf a_i^\\top\\big)\\otimes\\big(\\operatorname{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\big).\n",
        "$$\n",
        "\n",
        "Regularizer : Since $\\nabla_{\\mathbf X}(\\mu\\mathbf X)=\\mu\\mathbf I$ in the vector space,\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf X)\n",
        "=\\nabla^2 f(\\mathbf X)+\\mu\\,\\mathbf I\n",
        "=\\sum_{i=1}^n(\\mathbf a_i\\mathbf a_i^\\top)\\otimes\\big(\\operatorname{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\big)+\\mu\\,\\mathbf I.\n",
        "$$\n",
        "\n",
        "**PSD note.** $\\operatorname{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\succeq0$ (softmax Jacobian),  \n",
        "$\\mathbf a_i\\mathbf a_i^\\top\\succeq0$, and the Kronecker of PSDs is PSD; adding $\\mu\\mathbf I$ makes it strictly PD for $\\mu>0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Claim.** $f_\\mu$ is $\\mu$-strongly convex on $\\mathbb{R}^{p\\times C}$, i.e.,\n",
        "for all $\\mathbf X,\\mathbf Y$,\n",
        "$$\n",
        "f_\\mu(\\mathbf Y)\\;\\ge\\; f_\\mu(\\mathbf X) + \\langle \\nabla f_\\mu(\\mathbf X),\\, \\mathbf Y-\\mathbf X\\rangle\n",
        "\\;+\\; \\frac{\\mu}{2}\\,\\|\\mathbf Y-\\mathbf X\\|_F^2.\n",
        "$$\n",
        "\n",
        "**Proof via Hessian lower bound.** From (g),\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf X)\n",
        "=\\sum_{i=1}^n (\\mathbf a_i\\mathbf a_i^\\top)\\otimes\\big(\\mathrm{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\big)\n",
        "\\;+\\; \\mu\\,\\mathbf I.\n",
        "$$\n",
        "Each term $(\\mathbf a_i\\mathbf a_i^\\top)\\succeq 0$ and\n",
        "$\\mathrm{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\succeq 0$, hence their Kronecker product is PSD.\n",
        "Therefore,\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf X)\\;\\succeq\\; \\mu\\,\\mathbf I\\qquad\\text{for all }\\mathbf X,\n",
        "$$\n",
        "which is equivalent to $\\mu$-strong convexity.\n",
        "\n",
        "**Justification of positive semidefiniteness**\n",
        "\n",
        "1. **For** $(\\mathbf a_i \\mathbf a_i^\\top)$:\n",
        "\n",
        "For any vector $\\mathbf u$,\n",
        "$$\n",
        "\\mathbf u^\\top (\\mathbf a_i \\mathbf a_i^\\top) \\mathbf u\n",
        "= (\\mathbf a_i^\\top \\mathbf u)^2 \\ge 0.\n",
        "$$\n",
        "Hence, $\\mathbf a_i \\mathbf a_i^\\top \\succeq 0$.\n",
        "\n",
        "\n",
        "2. **For** $\\mathrm{Diag}(\\mathbf p_i) - \\mathbf p_i \\mathbf p_i^\\top$:\n",
        "\n",
        "For any $\\mathbf v \\in \\mathbb{R}^C$,\n",
        "$$\n",
        "\\mathbf v^\\top \\big(\\mathrm{Diag}(\\mathbf p_i) - \\mathbf p_i \\mathbf p_i^\\top\\big)\\mathbf v\n",
        "= \\sum_c p_{i,c} v_c^2 - \\Big(\\sum_c p_{i,c} v_c\\Big)^2\n",
        "= \\operatorname{Var}_{c\\sim \\mathbf p_i}(v_c) \\ge 0.\n",
        "$$\n",
        "Thus, the softmax Jacobian $\\mathrm{Diag}(\\mathbf p_i) - \\mathbf p_i \\mathbf p_i^\\top$ is positive semidefinite.\n",
        "\n",
        "\n",
        "\n",
        "3. **Kronecker product of PSD matrices is PSD:**\n",
        "\n",
        "Let $A\\succeq0$ and $B\\succeq0$.  \n",
        "For any vector $\\mathbf z$, reshape it as a matrix $Z$ such that $\\mathbf z=\\mathrm{vec}(Z)$. Then\n",
        "$$\n",
        "\\mathbf z^\\top (A \\otimes B)\\,\\mathbf z\n",
        "= \\mathrm{vec}(Z)^\\top (A\\otimes B)\\,\\mathrm{vec}(Z)\n",
        "= \\operatorname{tr}\\!\\big(B\\,Z^\\top A Z\\big).\n",
        "$$\n",
        "Using the PSD square roots $A^{1/2}$ and $B^{1/2}$,\n",
        "$$\n",
        "\\operatorname{tr}\\!\\big(B\\,Z^\\top A Z\\big)\n",
        "= \\operatorname{tr}\\!\\big(B^{1/2}(A^{1/2}Z)(A^{1/2}Z)^\\top B^{1/2}\\big)\n",
        "= \\|A^{1/2} Z B^{1/2}\\|_F^2 \\ge 0.\n",
        "$$\n",
        "Hence, $A\\otimes B \\succeq 0$.\n",
        "\n",
        "\n",
        "Applying this result with \n",
        "$$\n",
        "A = \\mathbf a_i \\mathbf a_i^\\top \\succeq 0, \\qquad\n",
        "B = \\mathrm{Diag}(\\mathbf p_i) - \\mathbf p_i \\mathbf p_i^\\top \\succeq 0,\n",
        "$$\n",
        "we conclude that each term \n",
        "$$\n",
        "(\\mathbf a_i \\mathbf a_i^\\top)\\otimes\\big(\\mathrm{Diag}(\\mathbf p_i) - \\mathbf p_i \\mathbf p_i^\\top\\big)\n",
        "$$\n",
        "is positive semidefinite.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(i)__ (1 point) Is it possible for a strongly convex function to not attain its minimum? Justify your reasoning (you may assume the domain is $\\mathbb{R}^{p\\times C}$).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        " No.  \n",
        "Let $f:\\mathbb{R}^{p\\times C}\\to\\mathbb{R}$ be a $\\mu$-strongly convex and continuous function.  \n",
        "\n",
        "\n",
        "\n",
        "From the definition of strong convexity, there exists a quadratic lower bound:\n",
        "$$\n",
        "f(\\mathbf{X}) \\;\\ge\\; g(\\mathbf{X})\n",
        "\\qquad\\text{for some quadratic } g(\\mathbf{X})\n",
        "= a + \\langle \\mathbf{b}, \\mathbf{X} \\rangle + \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2.\n",
        "$$\n",
        "In particular,\n",
        "$$\n",
        "f(\\mathbf{X}) \\;\\to\\; +\\infty \\quad \\text{as } \\|\\mathbf{X}\\|_F \\to \\infty.\n",
        "$$\n",
        "That is, $f$ is **coercive**.\n",
        "\n",
        "\n",
        "\n",
        "Hence, for any $\\varepsilon > 0$, there exists $R > 0$ such that\n",
        "$$\n",
        "\\forall\\, \\|\\mathbf{X}\\|_F \\ge R, \\quad\n",
        "f(\\mathbf{X}) \\;>\\; \\inf f + \\varepsilon.\n",
        "$$\n",
        "This means the infimum of $f$ is achieved inside the closed ball\n",
        "$$\n",
        "B_R = \\{\\mathbf{X} : \\|\\mathbf{X}\\|_F \\le R\\}.\n",
        "$$\n",
        "\n",
        "Since $f$ is continuous** and $B_R$ is compact in $\\mathbb{R}^{p\\times C}$,\n",
        "the Weierstrass theorem guarantees that $f$ attains its infimum on $B_R$.\n",
        "Therefore, $f$ attains its **minimum** on $\\mathbb{R}^{p\\times C}$.\n",
        "\n",
        "\n",
        "*(We also thought of using a Bolzano‚ÄìWeierstrass argument:\n",
        "any minimizing sequence is bounded, hence has a convergent subsequence,\n",
        "and continuity of $f$ ensures the limit attains the minimum.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now show that $f_\\mu$ is smooth, i.e., $\\nabla f_\\mu$ is L-Lipschitz with respect to the Frobenius norm, with a simple conservative bound\n",
        "$$\n",
        " L = \\|\\mathbf{A}\\|_F^2 + \\mu.\n",
        "$$\n",
        "where\n",
        "$$\n",
        " \\mathbf{A} = \\begin{bmatrix}\n",
        "  \\leftarrow &  \\mathbf{a}_1^\\top & \\rightarrow \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_2^\\top & \\rightarrow \\\\\n",
        "   &  \\ldots &  \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_n^\\top & \\rightarrow \\\\\n",
        " \\end{bmatrix}.\n",
        "$$\n",
        "(You may use that the operator norm of the softmax Jacobian is bounded by 1/4, and a looser bound $\\le 1$ is acceptable for grading.)\n",
        "\n",
        "Hint: check the properties of the spectral norm with respect to dot product, Kronecker product, and outer product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 point for all three questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "For $\\mathbf a \\in \\mathbb{R}^p$, the matrix $\\mathbf a \\mathbf a^\\top \\in \\mathbb{R}^{p\\times p}$ has **rank 1**.  \n",
        "Therefore, it has exactly **one nonzero eigenvalue**.\n",
        "\n",
        "Now, observe that\n",
        "$$\n",
        "(\\mathbf a \\mathbf a^\\top)\\mathbf a\n",
        "= \\mathbf a\\,(\\mathbf a^\\top \\mathbf a)\n",
        "= \\|\\mathbf a\\|_2^2\\,\\mathbf a.\n",
        "$$\n",
        "Hence, $\\mathbf a$ is an eigenvector of $\\mathbf a \\mathbf a^\\top$ with eigenvalue $\\|\\mathbf a\\|_2^2$.\n",
        "\n",
        "All other eigenvalues are zero, since the matrix has rank 1.  \n",
        "Therefore,\n",
        "$$\n",
        "\\lambda_{\\max}(\\mathbf a \\mathbf a^\\top) = \\|\\mathbf a\\|_2^2.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-2)__ Using (2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "We saw\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf X)\n",
        "=\\sum_{i=1}^n (\\mathbf a_i\\mathbf a_i^\\top)\\otimes\\big(\\mathrm{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top\\big) + \\mu\\,\\mathbf I.\n",
        "$$\n",
        "All summands are symmetric PSD\n",
        "1) $\\lambda_{\\max}\\!\\left(\\sum_j M_j\\right)\\le \\sum_j \\lambda_{\\max}(M_j)$ for PSD $M_j$.\n",
        "2) $\\|A\\otimes B\\|_2 = \\|A\\|_2\\,\\|B\\|_2$ (hence $\\lambda_{\\max}(A\\otimes B)=\\lambda_{\\max}(A)\\lambda_{\\max}(B)$ for PSD $A,B$).\n",
        "3) From (j‚Äì1), $\\lambda_{\\max}(\\mathbf a_i\\mathbf a_i^\\top)=\\|\\mathbf a_i\\|_2^2$.\n",
        "4) For the softmax Jacobian $J_i:=\\mathrm{Diag}(\\mathbf p_i)-\\mathbf p_i\\mathbf p_i^\\top$, $\\|J_i\\|_2\\le 1/4$ (and the looser bound $\\le 1$ is acceptable).\n",
        "\n",
        "Therefore,\n",
        "$$\n",
        "\\lambda_{\\max}\\big(\\nabla^2 f_\\mu(\\mathbf X)\\big)\n",
        "\\;\\le\\; \\sum_{i=1}^n \\lambda_{\\max}\\!\\big((\\mathbf a_i\\mathbf a_i^\\top)\\otimes J_i\\big) + \\mu\n",
        "\\;=\\; \\sum_{i=1}^n \\lambda_{\\max}(\\mathbf a_i\\mathbf a_i^\\top)\\,\\lambda_{\\max}(J_i) + \\mu\n",
        "\\;\\le\\; \\sum_{i=1}^n \\|\\mathbf a_i\\|_2^2 \\cdot 1 + \\mu.\n",
        "$$\n",
        "\n",
        "Hence,\n",
        "$$\n",
        "\\boxed{\\;\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf X)) \\le \\sum_{i=1}^n \\|\\mathbf a_i\\|_2^2 + \\mu\\;}\n",
        "$$\n",
        "(and with the sharper $\\|J_i\\|_2\\le 1/4$, the bound improves to $\\tfrac{1}{4}\\sum_i\\|\\mathbf a_i\\|_2^2+\\mu$).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|\\mathbf{A}\\|_F^2 + \\mu$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "From (j‚Äì2), for all $\\mathbf X$,\n",
        "$$\n",
        "\\lambda_{\\max}\\big(\\nabla^2 f_\\mu(\\mathbf X)\\big)\n",
        "\\;\\le\\; \\sum_{i=1}^n \\|\\mathbf a_i\\|_2^2 + \\mu.\n",
        "$$\n",
        "Since $\\mathbf A$ stacks the row vectors $\\mathbf a_i^\\top$, we have\n",
        "$$\n",
        "\\|\\mathbf A\\|_F^2 \\;=\\; \\sum_{i=1}^n \\|\\mathbf a_i\\|_2^2.\n",
        "$$\n",
        "Therefore,\n",
        "$$\n",
        "\\sup_{\\mathbf X}\\|\\nabla^2 f_\\mu(\\mathbf X)\\|_2\n",
        "\\;=\\; \\sup_{\\mathbf X}\\lambda_{\\max}\\big(\\nabla^2 f_\\mu(\\mathbf X)\\big)\n",
        "\\;\\le\\; \\|\\mathbf A\\|_F^2 + \\mu \\;=\\; L.\n",
        "$$\n",
        "\n",
        "For twice continuously differentiable functions, the bound\n",
        "$\\|\\nabla^2 f_\\mu(\\mathbf X)\\|_2 \\le L$ for all $\\mathbf X$\n",
        "is equivalent to **$L$-smoothness** of $f_\\mu$, i.e.,\n",
        "$$\n",
        "\\|\\nabla f_\\mu(\\mathbf Y)-\\nabla f_\\mu(\\mathbf X)\\|_F\n",
        "\\;\\le\\; L\\,\\|\\mathbf Y-\\mathbf X\\|_F,\n",
        "$$\n",
        "and (equivalently)\n",
        "$$\n",
        "f_\\mu(\\mathbf Y) \\;\\le\\; f_\\mu(\\mathbf X) + \\langle \\nabla f_\\mu(\\mathbf X), \\mathbf Y-\\mathbf X\\rangle\n",
        "+ \\frac{L}{2}\\,\\|\\mathbf Y-\\mathbf X\\|_F^2.\n",
        "$$\n",
        "Hence, $f_\\mu$ is $L$-smooth with $L=\\|\\mathbf A\\|_F^2+\\mu$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(l)__ (1 point) KL divergence and NLL. Let $q(b_i\\mid\\mathbf{a}_i)$ be the true label distribution and $p(b_i\\mid\\mathbf{a}_i)$ the model softmax. Write the KL divergence $\\mathrm{KL}(q\\,\\|\\,p)$ and show that minimizing the KL divergence between $q$ and $p$ is equivalent to minimizing the negative log-likelihood derived in (a).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let $q(b_i \\mid \\mathbf{a}_i)$ be the true label distribution and $p(b_i \\mid \\mathbf{a}_i)$ the model softmax.\n",
        "The Kullback‚ÄìLeibler divergence between $q$ and $p$ is:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{KL}(q\\,\\|\\,p)\n",
        "&= \\sum_{c=1}^C q(b_i = c \\mid \\mathbf{a}_i)\n",
        "   \\log\\!\\left(\\frac{q(b_i = c \\mid \\mathbf{a}_i)}{p(b_i = c \\mid \\mathbf{a}_i)}\\right)\\\\[4pt]\n",
        "&= \\sum_{c=1}^C q(b_i = c \\mid \\mathbf{a}_i)\\log q(b_i = c \\mid \\mathbf{a}_i)\n",
        " - \\sum_{c=1}^C q(b_i = c \\mid \\mathbf{a}_i)\\log p(b_i = c \\mid \\mathbf{a}_i).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The first term does **not depend** on the model parameters, so minimizing the KL divergence is equivalent to minimizing:\n",
        "$$\n",
        "-\\sum_{c=1}^C q(b_i = c \\mid \\mathbf{a}_i)\\,\\log p(b_i = c \\mid \\mathbf{a}_i).\n",
        "$$\n",
        "\n",
        "If the true distribution $q$ is **one-hot**, i.e.\n",
        "$$\n",
        "q(b_i = c \\mid \\mathbf{a}_i) = \\mathbf{1}_{\\{b_i = c\\}},\n",
        "$$\n",
        "then:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\arg\\min_p \\mathrm{KL}(q\\,\\|\\,p)\n",
        "&= \\arg\\min_p \\left( - \\sum_{c=1}^C \\mathbf{1}_{\\{b_i = c\\}} \\log p(b_i = c \\mid \\mathbf{a}_i) \\right)\\\\[4pt]\n",
        "&= \\arg\\min_p \\left( -\\log p(b_i \\mid \\mathbf{a}_i) \\right)\\\\[4pt]\n",
        "&= \\arg\\min_{\\mathbf X}\n",
        "\\left( -\\log \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_{b_i})}\n",
        "{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)} \\right)\\\\[4pt]\n",
        "&= \\arg\\min_{\\mathbf X}\n",
        "\\left( -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i}\n",
        "+ \\log\\!\\left(\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)\\right) \\right).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion:**  \n",
        "For a given $\\mathbf{a}_i$, minimizing the KL divergence $\\mathrm{KL}(q\\,\\|\\,p)$\n",
        "is equivalent to minimizing the **negative log-likelihood** (NLL) derived in part (a).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From your work in this section, you have shown that the maximum likelihood estimator for multiclass softmax logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_F^2$ regularizer. Consequently, the estimator for $\\mathbf{X}$ we will use will be the solution of the smooth strongly convex problem,\n",
        "$$\n",
        " \\mathbf{X}^\\star = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X}) + \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2.\\tag{3}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binary logistic regression (specialization for Part 2)\n",
        "\n",
        "While this part analyzed the multiclass (softmax) setting, in the next exercise we will continue under the simplified two-class case.\n",
        "\n",
        "Let labels be $b_i \\in \\{-1, +1\\}$, features $\\mathbf{a}_i \\in \\mathbb{R}^p$, and weight vector $\\mathbf{x} \\in \\mathbb{R}^p$. Define the sigmoid\n",
        "$$\n",
        "\\sigma(t) = \\frac{1}{1+e^{-t}}.\n",
        "$$\n",
        "Model the conditional distribution as\n",
        "$$\n",
        "\\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma\\big(j\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big), \\quad j \\in \\{-1,+1\\}.\n",
        "$$\n",
        "The likelihood over i.i.d. samples $\\{(\\mathbf{a}_i, b_i)\\}_{i=1}^n$ is\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\prod_{i=1}^n \\sigma\\big(b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big),\n",
        "$$\n",
        "so the negative log-likelihood is\n",
        "$$\n",
        " f(\\mathbf{x}) = -\\log \\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^n \\log\\big(1 + e^{-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
        "$$\n",
        "\n",
        "__(m)__ (1 point) Show that the gradient of the negative log-likelihood is the standard binary logistic regression gradient:\n",
        "$$\n",
        "\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\big(-b_i\\, \\sigma(-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x})\\big)\\, \\mathbf{a}_i.\n",
        "$$\n",
        "(Hint: use the chain rule and $\\sigma'(t) = \\sigma(t)\\big(1-\\sigma(t)\\big)$.)\n",
        "\n",
        "We will use this binary formulation in Part 2 - First order methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have\n",
        "$$\n",
        "f(\\mathbf{x})=\\sum_{i=1}^n \\log\\big(1+e^{-b_i\\,\\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
        "$$\n",
        "Let $u_i(\\mathbf{x})=-b_i\\,\\mathbf{a}_i^\\top \\mathbf{x}$. Then one term is\n",
        "$$\n",
        "\\phi_i(\\mathbf{x})=\\log\\big(1+e^{u_i}\\big).\n",
        "$$\n",
        "By the chain rule,\n",
        "$$\n",
        "\\nabla \\phi_i(\\mathbf{x})\n",
        "= \\frac{e^{u_i}}{1+e^{u_i}}\\,\\nabla u_i(\\mathbf{x})\n",
        "= \\sigma(u_i)\\,(-b_i\\,\\mathbf{a}_i)\n",
        "= \\big(-b_i\\,\\sigma(-b_i\\,\\mathbf{a}_i^\\top \\mathbf{x})\\big)\\,\\mathbf{a}_i.\n",
        "$$\n",
        "Summing over $i$ gives\n",
        "$$\n",
        "\\nabla f(\\mathbf{x})\n",
        "=\\sum_{i=1}^n \\big(-b_i\\,\\sigma(-b_i\\,\\mathbf{a}_i^\\top \\mathbf{x})\\big)\\,\\mathbf{a}_i.\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
